# agents.py

import httpx # CHANGEMENT : Importe httpx au lieu de requests
import time
import logging
# import asyncio # Plus besoin de asyncio ici car httpx est asynchrone

# Configuration du logger
logging.basicConfig(level=logging.INFO, format='[%(asctime)s] %(levelname)s %(message)s')

class Agent:
    def __init__(self, name, role, llm_model, description_for_orchestrator):
        self.name = name
        self.role = role
        self.llm_model = llm_model # C'est le nom du mod√®le Ollama personnalis√© (ex: "agent-architecte")
        self.description_for_orchestrator = description_for_orchestrator # Description utile pour l'orchestrateur
        self.memory = [] # Pour stocker l'historique de conversation de cet agent

    # CORRECTION CRUCIALE : aask est maintenant une fonction SYNCHRONE
    def aask(self, prompt, context_messages=None, timeout=180):
        """Envoie un prompt √† Ollama, g√®re les erreurs et fournit un fallback intelligent."""
        messages = []
        # Le SYSTEM prompt est d√©j√† int√©gr√© dans le Modelfile de chaque agent.
        # Ici, nous construisons juste les messages user/assistant pour la conversation.

        if context_messages:
            messages.extend(context_messages)
        
        # Ajout du prompt utilisateur
        messages.append({"role": "user", "content": prompt})

        logging.info(f"[Agent {self.name}] D√©but appel Ollama ({self.llm_model})...")
        start = time.time()
        try:
            # Utilise httpx.Client pour les requ√™tes synchrones, car cette fonction sera appel√©e via asyncio.to_thread
            client = httpx.Client() # CHANGEMENT : Utilise httpx.Client
            response = client.post( # CHANGEMENT : Appel synchrone
                "http://localhost:11434/api/chat", # Assurez-vous que votre serveur Ollama tourne sur ce port
                json={"model": self.llm_model, "messages": messages, "stream": False},
                timeout=timeout
            )
            response.raise_for_status()
            data = response.json()
            
            content = data.get("message", {}).get("content")
            if not content:
                content = data.get("content") or data.get("response") or "(Pas de r√©ponse valide de l'IA)"

            elapsed = time.time() - start
            logging.info(f"[Agent {self.name}] R√©ponse Ollama re√ßue en {elapsed:.2f}s")
            
            self.memory.append({"role": "user", "content": prompt})
            self.memory.append({"role": "assistant", "content": content})

            return content
        except httpx.TimeoutException: # CHANGEMENT : G√®re l'exception Timeout de httpx
            logging.warning(f"[Agent {self.name}] Timeout apr√®s {timeout}s pour {self.llm_model} - Fallback utilis√©.")
            return self._generate_generic_fallback(prompt)
        except httpx.RequestError as e: # CHANGEMENT : G√®re l'exception RequestError de httpx
            logging.error(f"Erreur de requ√™te Ollama pour {self.name} ({self.llm_model}): {e} - Fallback utilis√©.")
            return self._generate_generic_fallback(prompt)
        except Exception as e:
            logging.error(f"Erreur inattendue pour l'agent {self.name} ({self.llm_model}): {e} - Fallback utilis√©.")
            return self._generate_generic_fallback(prompt)

    def _generate_generic_fallback(self, prompt):
        """R√©ponse de fallback g√©n√©rique si l'appel LLM √©choue."""
        return (
            f"ü§ñ **{self.name} (Mod√®le {self.llm_model})**\n\n"
            f"Je suis d√©sol√©, je n'ai pas pu g√©n√©rer une r√©ponse compl√®te √† votre demande : \"{prompt[:100]}...\".\n\n"
            "Une erreur est survenue lors de la communication avec mon mod√®le d'IA. "
            "Veuillez r√©essayer plus tard ou reformuler votre demande."
        )

# --- INSTANCIATION DE TOUS VOS AGENTS SP√âCIALIS√âS (ANCIENS AGENTS RETIR√âS) ---
# Utilisez les noms de mod√®les Ollama que vous avez cr√©√©s via les modelfiles !
# Noms des instances (name) doivent √™tre uniques pour React et get_agent_by_name

agents = [
    # Agents fondamentaux (maintenant bas√©s sur Ollama)
    Agent(
        name="Mike", # Visionnaire dans votre workflow
        role="Team Leader / Visionnaire",
        llm_model="agent-visionnaire",
        description_for_orchestrator="Responsable de la coordination globale du projet et de la d√©finition de la vision initiale."
    ),
    Agent(
        name="Emma", # Peut √™tre l'expert contenu/SEO ou un manager de produit
        role="Product Manager / Expert Contenu",
        llm_model="agent-seo-content-expert",
        description_for_orchestrator="D√©finit les sp√©cifications fonctionnelles, les user stories et la roadmap produit."
    ),
    Agent(
        name="Bob", # Architecte
        role="Architecte Logiciel",
        llm_model="agent-architecte",
        description_for_orchestrator="Con√ßoit la structure technique de l'application (frontend, backend, BDD, services)."
    ),
    # L'ancien "Alex" l'Ing√©nieur g√©n√©raliste est remplac√© par FrontEngineer et BackEngineer
    # L'ancien "David" le Data Analyst est maintenant DBMaster et peut √™tre int√©gr√© par prompt
    
    # Agents sp√©cialis√©s (ceux que nous avons ajout√©s via Modelfiles)
    Agent(
        name="FrontEngineer",
        role="Ing√©nieur Frontend",
        llm_model="agent-frontend-engineer",
        description_for_orchestrator="√âcrit le code HTML, CSS, JavaScript et React pour l'interface utilisateur."
    ),
    Agent(
        name="BackEngineer",
        role="Ing√©nieur Backend",
        llm_model="agent-backend-engineer",
        description_for_orchestrator="D√©veloppe les APIs, la logique m√©tier c√¥t√© serveur et g√®re les interactions base de donn√©es."
    ),
    Agent(
        name="UIDesigner",
        role="Designer UI/UX",
        llm_model="agent-designer-ui-ux",
        description_for_orchestrator="Propose des designs visuels, des palettes de couleurs et des principes d'exp√©rience utilisateur."
    ),
    Agent(
        name="SEOCopty",
        role="Expert SEO / Contenu",
        llm_model="agent-seo-content-expert",
        description_for_orchestrator="G√©n√®re du contenu textuel optimis√© pour le SEO et propose des balises meta."
    ),
    Agent(
        name="DBMaster",
        role="Sp√©cialiste Base de Donn√©es",
        llm_model="agent-database-specialist",
        description_for_orchestrator="Con√ßoit les sch√©mas de base de donn√©es et √©crit les requ√™tes SQL/ORM."
    ),
    Agent(
        name="DevOpsGuy",
        role="D√©ployeur / DevOps",
        llm_model="agent-deployer-devops",
        description_for_orchestrator="Pr√©pare l'application pour le d√©ploiement et fournit les scripts DevOps."
    ),
    Agent(
        name="TheCritique",
        role="Critique Qualit√© & S√©curit√©",
        llm_model="agent-critique",
        description_for_orchestrator="Identifie les erreurs, les failles de s√©curit√© et les am√©liorations dans le code ou les plans."
    ),
    Agent(
        name="TheOptimizer",
        role="Optimiseur de Code",
        llm_model="agent-optimiseur",
        description_for_orchestrator="Optimise la performance, la lisibilit√© et la maintenabilit√© du code g√©n√©r√©."
    ),
    Agent(
        name="TranslatorBot",
        role="Traducteur",
        llm_model="agent-translator",
        description_for_orchestrator="Traduit le contenu textuel dans diff√©rentes langues."
    ),
]

# --- Fonction utilitaire pour trouver un agent par son nom ---
def get_agent_by_name(agent_name: str):
    """Recherche et retourne une instance d'agent par son nom."""
    for agent in agents:
        if agent.name == agent_name:
            return agent
    return None